# -*- coding: utf-8 -*-
"""ML hw 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kLbK57hyQ9CM3gJMC1d9xQ8kly1Fzmgw

Language Identification with Naive Bayes
"""

import numpy as np
import pandas as pd
import math

# character types
S = [chr(i) for i in range(ord('a'), ord('z') + 1)]
S.append(" ")

# labels
L = ['e', 'j', 's']

class NaiveBayes:

    def __init__(self, label, characters, files, alpha = 0.5):
        self.label = label
        self.characters = characters
        self.files = files
        self.alpha = alpha
        self.counts = self.get_counts()
        self.probabilities = self.get_probabilities()

    def get_counts(self):
        counts = np.zeros(len(self.characters))
        for i in range(len(self.files)):
            file = open("homework4/data/languageID/" + self.label + str(self.files[i]) + ".txt")
            data = file.read()
            for c in range(len(self.characters)):
                counts[c] = counts[c] + data.count(self.characters[c])
        return(counts)

    def get_probabilities(self):
        N = sum(self.counts)
        probs = []
        for i in range(len(self.characters)):
            p = (self.counts[i] + self.alpha) / (N + 27 * self.alpha)
            probs.append(p)
        return(probs)

    def get_log_lik(self, filename):
        X = np.zeros(len(self.characters))
        file = open("homework4/data/languageID/" + filename + ".txt")
        data = file.read()
        for c in range(len(self.characters)):
            X[c] = data.count(self.characters[c])
        P = self.probabilities
        return(sum([x * math.log(p) for p, x in zip(P, X)]))

    def print_table(self, type, n_dec=4):
        if type == 1:
            values = self.counts
        elif type == 2:
            values = self.probabilities
        for i in range(len(self.characters)):
            print(self.characters[i], "&", round(values[i], n_dec), "\\\\")

if __name__=='__main__':
    # # print theta tables
    # for l in range(len(L)):
    #     print(L[l])
    #     c = NaiveBayes(L[l], S, range(10))
    #     c.print_table(2)

    # # question 4
    # e = NaiveBayes("e", S, [10])
    # e.print_table(1)

    # # question 5
    # post = []
    # for l in range(len(L)):
    #     print(L[l])
    #     c = NaiveBayes(L[l], S, range(10))
    #     post.append(c.get_log_lik("e10"))
    #     print(post[l])

    # question 7
    models = []
    for m in range(len(L)):
        models.append(NaiveBayes(L[m], S, range(10)))

    print("predicted", "actual")
    for l in range(len(L)):
        for f in range(10, 20):
            name = L[l] + str(f)
            log_lik = []
            for m in range(len(models)):
                log_lik.append(models[m].get_log_lik(name))
            max_index = np.argmax(log_lik)
            print(L[max_index], L[l])

    pass

"""Feed-Forward"""

import numpy as np
from torchvision import datasets, transforms
from scipy.special import softmax, expit
import matplotlib.pyplot as plt

mnist_train = datasets.MNIST(
    root="homework4/data",
    download=True,
    train=True,
    transform=transforms.ToTensor()
    )
mnist_test = datasets.MNIST(
    root="homework4/data",
    download=True,
    train=False,
    transform=transforms.ToTensor()
    )

def get_inputs(data):
    X = np.zeros(len(data) * 28 * 28).reshape(len(data), 784)
    for i in range(len(data)):
        X[i] = np.array(data[i][0]).reshape(1, 784)[0]
    return(X.T)

def get_labels(data):
    y = np.zeros(len(data))
    for i in range(len(data)):
        y[i] = data[i][1]
    return(y)

X_train = get_inputs(mnist_train)
y_train = get_labels(mnist_train)
X_test = get_inputs(mnist_test)
y_test = get_labels(mnist_test)

def get_predictions(data, W1, W2, W3):
    return(softmax(np.matmul(W3, expit(np.matmul(W2, expit(np.matmul(W1, data)))))))

def get_loss(labels, predictions):
    loss = []
    for i in range(len(labels)):
        index = int(labels[i] - 1)
        y_hat = predictions[index, i]
        loss.append(np.log(y_hat))
    return(-sum(loss))

def get_one_hot_vectors(labels):
    mat = np.zeros((10 * len(labels))).reshape(10, len(labels))
    for i in range(len(labels)):
        index = int(labels[i] - 1)
        mat[index, i] = 1
    return(mat)

# initialize weights
W1 = np.random.uniform(low=0, high=1, size=(784 * 300)).reshape(300, 784)
W2 = np.random.uniform(low=0, high=1, size=(300 * 200)).reshape(200, 300)
W3 = np.random.uniform(low=0, high=1, size=(200 * 10)).reshape(10, 200)

# training
h = .000001
epochs = 10
it = 1
batch_size = 60
y = get_one_hot_vectors(y_train)

train_loss = [get_loss(y_train, get_predictions(X_train, W1, W2, W3)) / 60000]
test_loss = [get_loss(y_test, get_predictions(X_test, W1, W2, W3)) / 10000]
error = []

print(get_predictions(X_test[:, 0], W1, W2, W3))

while it <= epochs:
    batch_ids = np.random.choice(range(len(y_train)), size=60000, replace=False) # randint(low=0, high=60000, size=batch_size)
    batches = len(y_train) / batch_size
    for b in range(int(batches)):
        start = b * batch_size
        stop = start + batch_size
        ids = batch_ids[start:stop]
        X_batch = X_train[:, ids]
        y_batch = y[:, ids]
        a1 = expit(np.matmul(W1, X_batch))
        a2 = expit(np.matmul(W2, a1))
        g = get_predictions(X_batch, W1, W2, W3)
        d3 = (g - y_batch) # * a3 * (1 - a3)
        d2 = np.matmul(W3.T, d3) * a2 * (1 - a2)
        d1 = np.matmul(W2.T, d2) * a1 * (1 - a1)
        W3 = W3 - h * np.matmul(d3, a2.T)
        W2 = W2 - h * np.matmul(d2, a1.T)
        W1 = W1 - h * np.matmul(d1, X_batch.T)
    print(get_predictions(X_test[:, 0], W1, W2, W3))
    print(it, get_loss(y_train, get_predictions(X_train, W1, W2, W3)) / 60000)
    train_loss.append(get_loss(y_train, get_predictions(X_train, W1, W2, W3)) / 60000)
    test_loss.append(get_loss(y_test, get_predictions(X_test, W1, W2, W3)) / 10000)
    pred = get_predictions(X_test, W1, W2, W3)
    e = 0
    for i in range(len(y_test)):
        p = np.argmax(pred[:, i])
        if p != int(y_test[i]): e += 1
    error.append(e / len(y_test))
    it += 1

for i in range(len(error)):
    print(i+1, "&", round(error[i]*100, 4), "\%", "\\\\")

plt.plot(range(epochs + 1), train_loss, label='train loss')
plt.plot(range(epochs + 1), test_loss, label='test loss')
plt.legend(loc="upper right")
plt.title("Learning Curve")
plt.xlabel("epoch")
plt.ylabel("Average Loss")
plt.savefig("learning_curve.png")

if __name__ == '__main__':
    pass

"""Pytorch"""

import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

mnist_train = datasets.MNIST(
    root="homework4/data",
    download=True,
    train=True,
    transform=transforms.ToTensor(),
    target_transform=transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))
    )
mnist_test = datasets.MNIST(
    root="homework4/data",
    download=True,
    train=False,
    transform=transforms.ToTensor(),
    target_transform=transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))
    )

class NeuralNetwork(nn.Module):

    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.w1 = nn.Linear(28*28, 300)
        self.w2 = nn.Linear(300, 200)
        self.w3 = nn.Linear(200, 10)
        nn.init.uniform_(self.w1.weight, a=-1, b=1)
        nn.init.uniform_(self.w1.weight, a=-1, b=1)
        nn.init.uniform_(self.w1.weight, a=-1, b=1)
        self.stack = nn.Sequential(
            self.w1,
            nn.Sigmoid(),
            self.w2,
            nn.Sigmoid(),
            self.w3,
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.stack(x)
        return logits

model = NeuralNetwork()

def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

    plt_train_loss.append(loss.item())

def test_loop(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss, correct = 0, 0

    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(dim=1) == y.argmax(dim=1)).type(torch.float).sum().item()

    test_loss /= num_batches
    plt_test_loss.append(test_loss)
    correct /= size
    test_errors.append(correct)
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
    print(f"Avg loss: {test_loss:>8f} \n")

# hyperparameters
learning_rate = .01
batch_size = 64
epochs = 20

train_dataloader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

plt_train_loss = []
plt_test_loss = []

test_errors = []

for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train_loop(train_dataloader, model, loss_fn, optimizer)
    test_loop(test_dataloader, model, loss_fn)
print("Done!")

for i in range(epochs):
    print(i+1, "&", round((1 - test_errors[i])*100, 4), "\%", "\\\\")

plt.plot(range(len(plt_train_loss)), plt_train_loss, label='train loss')
plt.plot(range(len(plt_test_loss)), plt_test_loss, label='test loss')
plt.legend(loc="upper right")
plt.title("Learning Curve")
plt.xlabel("epoch")
plt.ylabel("Average Loss")
# plt.savefig("homework4/figs/pytorch_learning_curve.png")
# plt.savefig("homework4/figs/init_zeros_learning_curve.png")
plt.savefig("neg_one_one_learning_curve.png")

if __name__ == '__main__':
    pass